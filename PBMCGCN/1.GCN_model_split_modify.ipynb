{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Flatten\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import sparse_categorical_accuracy\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adagrad\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from spektral.data import Dataset, Graph\n",
    "from spektral.data import MixedLoader\n",
    "from spektral.utils import normalized_laplacian\n",
    "from spektral.layers import GCNConv, GlobalSumPool, ChebConv, GlobalAttnSumPool\n",
    "from spektral.layers.ops import sp_matrix_to_sp_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./checkpoints/train/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-define dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class scRNA(Dataset):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.a = None\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def read(self):\n",
    "        x, y, node_order = _get_scRNA_exprs()\n",
    "        self.a = _get_adjacency(node_order)\n",
    "\n",
    "        return [Graph(x=x_, y=y_) for x_, y_ in zip(x, y)]\n",
    "\n",
    "def _get_adjacency(node_order):\n",
    "    #Read in edge file\n",
    "    g = nx.read_adjlist(\"../5.1.Edge_of_gene_gene_interaction_network.csv\",\n",
    "                        delimiter = \",\")\n",
    "    # Adjacency\n",
    "    A = nx.adj_matrix(g, weight = None, nodelist = node_order)\n",
    "    return A\n",
    "\n",
    "def _get_scRNA_exprs():\n",
    "    #Read in exprs file\n",
    "    exprs = pd.read_hdf(\"../6.Filtered_node_exprs.h5\", key = \"exprs\")\n",
    "    cell = pd.read_csv(\"../3.Cell_label.csv\", index_col = 0)\n",
    "    #Transpose the dataframe\n",
    "    exprs = exprs.T\n",
    "    #Exprs values reshape\n",
    "    x = exprs.values.reshape(exprs.shape[0], exprs.shape[1], 1)\n",
    "    #Cell label\n",
    "    y = cell[\"Number_label\"].values\n",
    "    #Node order\n",
    "    node_order = exprs.columns.to_list()\n",
    "    return x, y, node_order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed = 203)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading self defined dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = scRNA()\n",
    "\n",
    "# The adjacency matrix is stored as an attribute of the dataset.\n",
    "# Create filter for GCN and convert to sparse tensor.\n",
    "\n",
    "#data.a = normalized_laplacian(data.a)\n",
    "data.a = ChebConv.preprocess(data.a)\n",
    "#data.a = GCNConv.preprocess(data.a)\n",
    "data.a = sp_matrix_to_sp_tensor(data.a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "batch_size = 32  # Batch size\n",
    "epochs = 500  # Number of training epochs\n",
    "patience = 20  # Patience for early stopping\n",
    "l2_reg = 9e-3  # Regularization rate for l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset spilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = pd.read_csv(\"../3.Cell_label.csv\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_position = np.array([], dtype = int)\n",
    "testing_position = np.array([], dtype = int)\n",
    "validation_position = np.array([], dtype = int)\n",
    "\n",
    "for i in range(0,8):\n",
    "    if i < 1:\n",
    "        cell_numbers = cell[\"Number_label\"].value_counts()[i]\n",
    "        start = 0\n",
    "        end = cell_numbers\n",
    "        cell_total = np.array(range(start, end))\n",
    "        tmp_train = np.random.choice(cell_total, int(cell_numbers * 0.7), replace = False)\n",
    "        tmp = np.setdiff1d(cell_total, tmp_train)\n",
    "        tmp_validation = np.random.choice(tmp, int(cell_numbers * 0.2), replace = False)\n",
    "        tmp_testing = np.setdiff1d(tmp, tmp_validation)\n",
    "        train_position = np.append(train_position, tmp_train)\n",
    "        validation_position = np.append(validation_position, tmp_validation)\n",
    "        testing_position = np.append(testing_position, tmp_testing)\n",
    "        \n",
    "    else:\n",
    "        cell_numbers = cell[\"Number_label\"].value_counts()[i]\n",
    "        start = end \n",
    "        end = end + cell_numbers\n",
    "        cell_total = np.array(range(start, end))\n",
    "        tmp_train = np.random.choice(cell_total, int(cell_numbers * 0.7), replace = False)\n",
    "        tmp = np.setdiff1d(cell_total, tmp_train)\n",
    "        tmp_validation = np.random.choice(tmp, int(cell_numbers * 0.2), replace = False)\n",
    "        tmp_testing = np.setdiff1d(tmp, tmp_validation)\n",
    "        train_position = np.append(train_position, tmp_train)\n",
    "        validation_position = np.append(validation_position, tmp_validation)\n",
    "        testing_position = np.append(testing_position, tmp_testing)\n",
    "        \n",
    "del start, end, cell_total, tmp, tmp_train, tmp_testing, tmp_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data[train_position]\n",
    "testing = data[testing_position]\n",
    "valid = data[validation_position]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cell in train set: 59793\n",
      "Number of cell in validation set: 17081\n",
      "Number of cell in testing set: 8549\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of cell in train set: \" + str(train.n_graphs))\n",
    "print(\"Number of cell in validation set: \" + str(valid.n_graphs))\n",
    "print(\"Number of cell in testing set: \" + str(testing.n_graphs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The data need to be predicted\n",
    "Donor_A = data[85423:88323]\n",
    "Donor_C = data[88323:97842]\n",
    "Data6k = data[97842:103261]\n",
    "Data8k = data[103261:111642]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use a MixedLoader since the dataset is in mixed mode\n",
    "loader_tr = MixedLoader(train, \n",
    "                        batch_size = batch_size, epochs = epochs)\n",
    "loader_va = MixedLoader(valid, \n",
    "                        batch_size = batch_size)\n",
    "loader_te = MixedLoader(testing, \n",
    "                        batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "class Net(Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.conv1 = ChebConv(32, K = 5, activation = \"elu\", kernel_regularizer = l2(l2_reg))\n",
    "        self.bn0 = BatchNormalization()\n",
    "        self.conv2 = ChebConv(32, K = 5, activation = \"elu\", kernel_regularizer = l2(l2_reg))\n",
    "        self.bn1 = BatchNormalization()\n",
    "        #self.conv1 = GCNConv(32, activation = \"elu\", kernel_regularizer = l2(l2_reg))\n",
    "        #self.conv2 = GCNConv(32, activation = \"elu\", kernel_regularizer = l2(l2_reg))\n",
    "        #self.conv3 = GCNConv(32, activation = \"elu\", kernel_regularizer = l2(l2_reg))\n",
    "        #self.conv4 = GCNConv(32, activation = \"elu\", kernel_regularizer = l2(l2_reg))\n",
    "\n",
    "        # 參考 https://www.kaggle.com/kmader/mnist-graph-deep-learning\n",
    "        \n",
    "        self.flatten = Flatten()\n",
    "        \n",
    "        #Flattten有281280\n",
    "        self.fc1 = Dense(1024, activation = \"relu\", kernel_regularizer = l2(l2_reg))\n",
    "        self.fc2 = Dense(256, activation = \"relu\", kernel_regularizer = l2(l2_reg))\n",
    "        self.fc3 = Dense(64, activation = \"relu\", kernel_regularizer = l2(l2_reg))\n",
    "        self.fc4 = Dense(8, activation = \"softmax\")  # scRNA-seq has 8 classes\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, a = inputs\n",
    "        \n",
    "        x = self.conv1([x, a])\n",
    "        x = self.bn0(x)\n",
    "        x = self.conv2([x, a])\n",
    "        x = self.bn1(x)\n",
    "        output = self.flatten(x)\n",
    "        output = self.fc1(output)\n",
    "        output = self.fc2(output)\n",
    "        output = self.fc3(output)\n",
    "        output = self.fc4(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "GNN_model = Net()\n",
    "GNN_optimizer = SGD(learning_rate = 0.001)\n",
    "loss_fn = SparseCategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/train\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(GNN_model = GNN_model,\n",
    "                           GNN_optimizer = GNN_optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep = 2)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training method define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_total_loss = np.array([])\n",
    "training_acc = np.array([])\n",
    "training_loss = np.array([])\n",
    "model_loss = np.array([])\n",
    "validation_loss = np.array([])\n",
    "validation_acc = np.array([])\n",
    "testing_loss = np.array([])\n",
    "testing_acc = np.array([])\n",
    "\n",
    "# Training function\n",
    "@tf.function\n",
    "def train_on_batch(inputs, target):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = GNN_model(inputs, training = True)\n",
    "        train_losses = loss_fn(target, predictions)\n",
    "        model_losses = sum(GNN_model.losses)\n",
    "        loss = train_losses + model_losses\n",
    "        acc = tf.reduce_mean(sparse_categorical_accuracy(target, predictions))\n",
    "\n",
    "    gradients = tape.gradient(loss, GNN_model.trainable_variables)\n",
    "    GNN_optimizer.apply_gradients(zip(gradients, GNN_model.trainable_variables))\n",
    "    return loss, acc, train_losses, model_losses\n",
    "\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(loader):\n",
    "    step = 0\n",
    "    results = []\n",
    "    for batch in loader:\n",
    "        step += 1\n",
    "        inputs, target = batch\n",
    "        predictions = GNN_model(inputs, training = False)\n",
    "        loss = loss_fn(target, predictions)\n",
    "        acc = tf.reduce_mean(sparse_categorical_accuracy(target, predictions))\n",
    "        results.append((loss, acc, len(target)))  # Keep track of batch size\n",
    "        if step == loader.steps_per_epoch:\n",
    "            results = np.array(results)\n",
    "            return np.average(results[:, :-1], 0, weights = results[:, -1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 147.80355381965637s \n",
      "Total train loss: 22.9112, acc: 0.8347 | Train loss: 0.4088 | Model loss: 22.5025 | Valid loss: 0.3142, acc: 0.8687 | Test loss: 0.3190, acc: 0.8685\n",
      "Saving checkpoint for epoch 0 sucessfully.\n",
      "Epoch 1: 129.75979161262512s \n",
      "Total train loss: 21.1969, acc: 0.9460 | Train loss: 0.1487 | Model loss: 21.0482 | Valid loss: 0.3101, acc: 0.8704 | Test loss: 0.3094, acc: 0.8732\n",
      "Saving checkpoint for epoch 1 sucessfully.\n",
      "Epoch 2: 138.35864305496216s \n",
      "Total train loss: 19.7552, acc: 0.9866 | Train loss: 0.0667 | Model loss: 19.6885 | Valid loss: 0.3244, acc: 0.8720 | Test loss: 0.3094, acc: 0.8732\n",
      "Saving checkpoint for epoch 2 sucessfully.\n",
      "Epoch 3: 122.41925001144409s \n",
      "Total train loss: 18.4490, acc: 0.9986 | Train loss: 0.0332 | Model loss: 18.4159 | Valid loss: 0.3405, acc: 0.8747 | Test loss: 0.3094, acc: 0.8732\n",
      "Saving checkpoint for epoch 3 sucessfully.\n",
      "Epoch 4: 122.46817183494568s \n",
      "Total train loss: 17.2459, acc: 0.9999 | Train loss: 0.0214 | Model loss: 17.2244 | Valid loss: 0.3354, acc: 0.8767 | Test loss: 0.3094, acc: 0.8732\n",
      "Saving checkpoint for epoch 4 sucessfully.\n",
      "Epoch 5: 122.63788270950317s \n",
      "Total train loss: 16.1265, acc: 1.0000 | Train loss: 0.0169 | Model loss: 16.1096 | Valid loss: 0.3380, acc: 0.8772 | Test loss: 0.3094, acc: 0.8732\n",
      "Saving checkpoint for epoch 5 sucessfully.\n",
      "Epoch 6: 122.42671179771423s \n",
      "Total train loss: 15.0818, acc: 1.0000 | Train loss: 0.0150 | Model loss: 15.0667 | Valid loss: 0.3396, acc: 0.8768 | Test loss: 0.3094, acc: 0.8732\n",
      "Saving checkpoint for epoch 6 sucessfully.\n",
      "Epoch 7: 122.67879366874695s \n",
      "Total train loss: 14.1058, acc: 1.0000 | Train loss: 0.0142 | Model loss: 14.0916 | Valid loss: 0.3414, acc: 0.8759 | Test loss: 0.3094, acc: 0.8732\n",
      "Saving checkpoint for epoch 7 sucessfully.\n",
      "Epoch 8: 122.52756905555725s \n",
      "Total train loss: 13.1935, acc: 1.0000 | Train loss: 0.0138 | Model loss: 13.1798 | Valid loss: 0.3424, acc: 0.8768 | Test loss: 0.3094, acc: 0.8732\n",
      "Saving checkpoint for epoch 8 sucessfully.\n",
      "Epoch 9: 122.7254068851471s \n",
      "Total train loss: 12.3409, acc: 1.0000 | Train loss: 0.0136 | Model loss: 12.3273 | Valid loss: 0.3436, acc: 0.8771 | Test loss: 0.3094, acc: 0.8732\n",
      "Saving checkpoint for epoch 9 sucessfully.\n",
      "Epoch 10: 122.67393350601196s \n",
      "Total train loss: 11.5439, acc: 1.0000 | Train loss: 0.0135 | Model loss: 11.5303 | Valid loss: 0.3439, acc: 0.8768 | Test loss: 0.3094, acc: 0.8732\n",
      "Saving checkpoint for epoch 10 sucessfully.\n",
      "Epoch 11: 124.55819821357727s \n",
      "Total train loss: 10.7988, acc: 1.0000 | Train loss: 0.0135 | Model loss: 10.7853 | Valid loss: 0.3433, acc: 0.8774 | Test loss: 0.3094, acc: 0.8732\n",
      "Saving checkpoint for epoch 11 sucessfully.\n",
      "Epoch 12: 123.85151243209839s \n",
      "Total train loss: 10.1023, acc: 1.0000 | Train loss: 0.0136 | Model loss: 10.0887 | Valid loss: 0.3458, acc: 0.8773 | Test loss: 0.3094, acc: 0.8732\n",
      "Saving checkpoint for epoch 12 sucessfully.\n",
      "Epoch 13: 122.89808797836304s \n",
      "Total train loss: 9.4512, acc: 1.0000 | Train loss: 0.0136 | Model loss: 9.4375 | Valid loss: 0.3446, acc: 0.8766 | Test loss: 0.3094, acc: 0.8732\n",
      "Saving checkpoint for epoch 13 sucessfully.\n",
      "Epoch 14: 122.86129260063171s \n",
      "Total train loss: 8.8424, acc: 1.0000 | Train loss: 0.0137 | Model loss: 8.8288 | Valid loss: 0.3464, acc: 0.8775 | Test loss: 0.3094, acc: 0.8732\n",
      "Saving checkpoint for epoch 14 sucessfully.\n",
      "Epoch 15: 122.65553736686707s \n",
      "Total train loss: 8.2734, acc: 1.0000 | Train loss: 0.0138 | Model loss: 8.2597 | Valid loss: 0.3469, acc: 0.8781 | Test loss: 0.3094, acc: 0.8732\n",
      "Saving checkpoint for epoch 15 sucessfully.\n",
      "Epoch 16: 122.89961838722229s \n",
      "Total train loss: 7.7414, acc: 1.0000 | Train loss: 0.0137 | Model loss: 7.7277 | Valid loss: 0.3480, acc: 0.8787 | Test loss: 0.3094, acc: 0.8732\n",
      "Saving checkpoint for epoch 16 sucessfully.\n",
      "Epoch 17: 122.81368589401245s \n",
      "Total train loss: 7.2440, acc: 1.0000 | Train loss: 0.0138 | Model loss: 7.2303 | Valid loss: 0.3523, acc: 0.8761 | Test loss: 0.3094, acc: 0.8732\n",
      "Saving checkpoint for epoch 17 sucessfully.\n",
      "Epoch 18: 122.93740844726562s \n",
      "Total train loss: 6.7790, acc: 1.0000 | Train loss: 0.0138 | Model loss: 6.7653 | Valid loss: 0.3576, acc: 0.8747 | Test loss: 0.3094, acc: 0.8732\n",
      "Saving checkpoint for epoch 18 sucessfully.\n",
      "Epoch 19: 122.56402230262756s \n",
      "Total train loss: 6.3443, acc: 1.0000 | Train loss: 0.0137 | Model loss: 6.3305 | Valid loss: 0.3543, acc: 0.8784 | Test loss: 0.3094, acc: 0.8732\n",
      "Saving checkpoint for epoch 19 sucessfully.\n",
      "Epoch 20: 123.70962476730347s \n",
      "Total train loss: 5.9377, acc: 1.0000 | Train loss: 0.0136 | Model loss: 5.9241 | Valid loss: 0.3519, acc: 0.8781 | Test loss: 0.3094, acc: 0.8732\n",
      "Saving checkpoint for epoch 20 sucessfully.\n",
      "Early stopping\n",
      "CPU times: user 12min 34s, sys: 54.7 s, total: 13min 28s\n",
      "Wall time: 45min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Setup training\n",
    "best_val_loss = 999999\n",
    "current_patience = patience\n",
    "step = 0\n",
    "epochs = 0\n",
    "# Training loop\n",
    "results_tr = []\n",
    "start = time.time()\n",
    "for batch in loader_tr:\n",
    "    step += 1\n",
    "\n",
    "    # Training step\n",
    "    inputs, target = batch\n",
    "    loss, acc, train_lossed, model_losses = train_on_batch(inputs, target)\n",
    "    results_tr.append((loss, acc, train_lossed, model_losses, len(target)))\n",
    "    \n",
    "    if step == loader_tr.steps_per_epoch:\n",
    "        results_va = evaluate(loader_va)\n",
    "        if results_va[0] < best_val_loss:\n",
    "            best_val_loss = results_va[0]\n",
    "            current_patience = patience\n",
    "            results_te = evaluate(loader_te)\n",
    "        else:\n",
    "            current_patience -= 1\n",
    "            if current_patience == 0:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "        # Print results\n",
    "        results_tr = np.array(results_tr)\n",
    "        results_tr = np.average(results_tr[:, :-1], 0, weights = results_tr[:, -1])\n",
    "        training_total_loss = np.append(training_total_loss, results_tr[0])\n",
    "        training_acc = np.append(training_acc, results_tr[1])\n",
    "        training_loss = np.append(training_loss, results_tr[2])\n",
    "        model_loss = np.append(model_loss, results_tr[3])\n",
    "        validation_loss = np.append(validation_loss, results_va[0])\n",
    "        validation_acc = np.append(validation_acc, results_va[1])\n",
    "        testing_loss = np.append(testing_loss, results_te[0])\n",
    "        testing_acc = np.append(testing_acc, results_te[1])\n",
    "        print(\"Epoch {}: {}s \\n\"\n",
    "              \"Total train loss: {:.4f}, acc: {:.4f} | Train loss: {:.4f} | Model loss: {:.4f} | \"\n",
    "              \"Valid loss: {:.4f}, acc: {:.4f} | \"\n",
    "              \"Test loss: {:.4f}, acc: {:.4f}\".format(epochs, time.time() - start ,\n",
    "                                                      *results_tr, *results_va, *results_te))\n",
    "\n",
    "        # Reset epoch\n",
    "        results_tr = []\n",
    "        step = 0\n",
    "        start = time.time()\n",
    "        # Save model\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print (\"Saving checkpoint for epoch {} sucessfully.\".format(epochs))\n",
    "        epochs += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./1.Training_history.csv\", \"w\") as outfile:\n",
    "    outfile.write(\"{}, {}, {}, {}, {}, {}, {}, {}, {}\\n\".format(\"Epochs\", \"Training_total_loss\", \n",
    "                                                                \"Training_accuracy\",\"Training_loss\", \"Model_loss\",\n",
    "                                                                \"Validation_loss\",\"Validation_accuracy\",\n",
    "                                                                \"Testing_loss\", \"Testing_accuracy\"))\n",
    "    for i in range(len(training_loss)):\n",
    "        outfile.write(\"{}, {}, {}, {}, {}, {}, {}, {}, {}\\n\".format(i, training_total_loss[i], training_acc[i],\n",
    "                                                                    training_loss[i], model_loss[i],  \n",
    "                                                                    validation_loss[i], validation_acc[i],\n",
    "                                                                    testing_loss[i], testing_acc[i]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
